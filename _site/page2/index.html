<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->
<head>
<meta charset="utf-8">
<title>why so serious?</title>
<meta name="description" content="main page.">
<meta name="keywords" content="">


<!-- Open Graph -->

<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="why so serious?">
<meta property="og:description" content="main page.">
<meta property="og:url" content="http://lambda.hk/page2/index.html">
<meta property="og:site_name" content="why so serious?">





<link rel="canonical" href="http://lambda.hk/page2/">
<link href="http://lambda.hk/feed.xml" type="application/atom+xml" rel="alternate" title="why so serious? Feed">


<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="http://lambda.hk/assets/css/main.min.css">
<link rel="stylesheet" href="http://lambda.hk/assets/css/main.css">
<!-- Webfonts -->
<link href="http://fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic" rel="stylesheet" type="text/css">

<meta http-equiv="cleartype" content="on">

<!-- Load Modernizr -->
<script src="http://lambda.hk/assets/js/vendor/modernizr-2.6.2.custom.min.js"></script>

<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="http://lambda.hk/favicon.ico">
<!-- 32x32 -->
<link rel="shortcut icon" href="http://lambda.hk/favicon.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="http://lambda.hk/images/lambda57.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="http://lambda.hk/images/lambda72.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="http://lambda.hk/images/lambda114.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://lambda.hk/images/lambda144.png">



</head>

<body id="post-index" class="feature">

<!--[if lt IE 9]><div class="upgrade"><strong><a href="http://whatbrowser.org/">Your browser is quite old!</strong> Why not upgrade to a different browser to better enjoy this site?</a></div><![endif]-->
<nav id="dl-menu" class="dl-menuwrapper" role="navigation">
	<button class="dl-trigger">Open Menu</button>
	<ul class="dl-menu">
		<li><a href="http://lambda.hk">Home</a></li>
		<li>
			<a href="#">About</a>
			<ul class="dl-submenu">
				<li>
					<img src="http://lambda.hk/images/avatar.png" alt="Pathfinder Mok photo" class="author-photo">
					<h4>Pathfinder Mok</h4>
					<p>Coding  &&  Reading</p>
					<p>Cycling &&  Hiking</p>
				</li>
				<!--li><a href="http://lambda.hk/about/">Learn More</a></li-->
				<li>
					<a href="mailto:xtcmhs@gmail.com"><i class="fa fa-envelope"></i> Email</a>
				</li>
				
				
				
				
				<li>
					<a href="http://github.com/pathfinder007"><i class="fa fa-github"></i> GitHub</a>
				</li>
				
				
				
				
				<li>
					<a href="http://www.renren.com/240122302/profile"><i class="fa fa-renren"></i> Renren</a>
				</li>
			</ul><!-- /.dl-submenu -->
		</li>
		<li>
			<a href="#">Posts</a>
			<ul class="dl-submenu">
				<li><a href="http://lambda.hk/posts/">All Posts</a></li>
				<li><a href="http://lambda.hk/tags/">All Tags</a></li>
			</ul>
		</li>
		<li><a href="http://lambda.hk"></a></li><li><a href="" target="_blank"></a></li>
	</ul><!-- /.dl-menu -->
</nav><!-- /.dl-menuwrapper -->


<div class="entry-header">
  
    <div class="entry-image">
      <img src="http://lambda.hk/images/abstract-10.jpg" alt="">
    </div><!-- /.entry-image -->
  
  <div class="header-title">
    <div class="header-title-wrap">
      <h1>why so serious?</h1>
      <h2></h2>
    </div><!-- /.header-title-wrap -->
  </div><!-- /.header-title -->
</div><!-- /.entry-header -->

<div id="main" role="main">
  <div class="right-nav">
    <div class="nav-unit">
	<h4>recent articles</h4>
    </div>
    <div class="visitor">
        <h4>visitors</h4>
    </div>
    <div class="the-map">
    <embed src="//rh.revolvermaps.com/f/g.swf" type="application/x-shockwave-flash" pluginspage="http://www.macromedia.com/go/getflashplayer" quality="high" wmode="window" allowScriptAccess="always" allowNetworking="all" width="220" height="220" flashvars="m=5&amp;i=78kqxwg8bw0&amp;r=false&amp;v=true&amp;b=000000&amp;n=false&amp;s=220&amp;c=ff0000"></embed><br /><img src="//rh.revolvermaps.com/js/c/78kqxwg8bw0.gif" width="1" height="1" alt="" />
    </div>
</div>
<a href="" target="_self" id="toTop" title="返回顶部" onclick="window.scrollTo(0,0);return false" style="display:none;"></a>
<script src="http://lambda.hk/assets/js/backtop.js"></script>

  
<article class="hentry">
  <header>
    <div class="entry-meta">
      <span class="entry-date date published updated"><time datetime="2014-10-22T00:00:00-04:00"><a href="http://lambda.hk/machine_learning/Neural-Network-Accelerator-II/">October 22, 2014</a></time></span><span class="author vcard"></span>&nbsp; &bull; &nbsp;<span class="entry-comments"><a href="http://lambda.hk/machine_learning/Neural-Network-Accelerator-II/#disqus_thread">Comment</a></span>
      
    </div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://lambda.hk/machine_learning/Neural-Network-Accelerator-II/" rel="bookmark" title="Neural Network Accelerator II" itemprop="url">Neural Network Accelerator II</a></h1>
    
  </header>
  <div class="entry-content">
    <h3 id="section">写在前面</h3>
<p>  这段时间由于工作需要，研究了一下ASPLOS’14的Best Paper，”Dianao: A Small-Footprint High-Throughput Accelerator for Ubiquitous Machine-Learning.” 这是中国大陆的学者首次获得 CCF 推荐的体系结构领域 A 类会议的最佳论文奖。而此前,ASPLOS 最佳论文 奖一直被美国卡耐基梅隆大学、德州大学奥斯丁分校、微软等 8 个欧美著名研究机构所垄断。下面结合这篇文章，写写阅读摘要。</p>

<h3 id="section-1">神经网络协处理器的发展.</h3>
<p>  现在的大多数协处理器研究者，关注的都是如何有效提高神经网络运算的效率。现今的深度神经网络，在规模上差别很大。寒武纪1号神经网络协处理器，强调存储对协处理器性能，功耗的决定性影响，通过一种优化的访存方式，解决了访存瓶颈问题。在台积电 65nm 工艺下,寒武纪 1 号主频可达 0.98GHz、性能 452 GOPS、功耗 0.485W、面积约为 3mm2。也就是说，它可以在通用处理器核 1/10 的面积和功耗开销下，达到通用处理器核 100 倍以上的人工神经网络处理速度。即便和最先进的 GPU 相比, 寒武纪 1 号的人工神经网络处理速度也不落下风，而其面积和功耗远低于 GPU 的 1/100。这样一个小面积，大吞吐量的机器学习协处理器，有着广泛的系统及产品级的应用前景。</p>

<p>  当处理器架构发展到异构多核，无论从提高运算性能还是降低功耗的角度来说，设计协处理器，都是一个广泛采用的解决方案。那么，在什么应用领域，有了设计协处理器的需求？是在很多对高性能运算有迫切需求的应用领域，从图像，视频，语音识别到自动翻译领域，商业分析，以及各种各样依赖于机器学习技术的领域。在大部分的领域，都有机器学习的用武之地，CNN/DNN在过去几年成为了最火的机器学习算法，很多的应用领域，都可以套用深度学习的模型，取得良好的效果。即一种算法，可以解决若干应用问题，这是一个独特的设计神经网络协处理器的时机，为几种主流的算法，定制专用的运算平台，这是一个潜力巨大的未占领市场。</p>

<p>  如今，大多数的深度学习运算任务，都在多核SIMD处理器，GPU集群或者FPGA上面执行，这是目前为止比较常用的解决方案。而有一些研究者已经提出了使用协处理器实现CNN或者MLP算法。协处理器应用的其他领域，像图像识别，也有用ASIC设计的方法实现卷积神经网络，或者其他典型的神经网络算法。但是这些协处理器实现，都是着眼于如何高效地实现运算，但它们或者因为设计简单，忽略了内存传输方面地问题，或者直接把协处理器通过DMA加到了内存上。并没有解决通用处理器集群运行神经网络算法受限的访存瓶颈问题。</p>

<h3 id="section-2">神经网络训练问题</h3>

<p>  我们都知道，神经网络算法的训练代价很大，无论是浅层神经网络的随机梯度下降，还是深度学习的Wake-Sleep，在大量样本的情况下，都需要特别长的模型训练时间。于是很多的神经网络算法研究者，有了提高训练效率的需求，当然，这只是一小部分的市场。而有更多的最终用户，需要更好的产品体验，诸如图像识别，语音识别，广告推荐，等等。而深度学习在这些领域，受限于硬件瓶颈，还并未得到很好的应用。这是一块巨大的市场。一个革新的硬件平台，可以大大加速深度学习技术在各个领域的应用。</p>

<p>&amp;  很多时候，我们都强调在线训练，在线调整模型的重要性，实际上，目前的大多数业界机器学习应用，都尚未做到在线实时调整模型，似乎也没有这个必要。毕竟增加小量样本的情况下，进行模型训练，有一个精确度与代价的权衡问题。我们会发现，在电商购买了某商品以后，多天之后，其网站还会给我们推荐同样的商品，这应该是模型并未重新训练的结果。一般大样本训练的模型，都会在增加新样本以后，通过一种增量训练的方法，调整模型(增量训练的方法，一直还没有去调研，mark一下)。所以，在线训练，对目前的深度学习应用来说，并不是显得那么的重要。</p>

<h3 id="cnndnn-">CNN/DNN 一般结构</h3>
<p>  CNN/DNN算法，由一系列的各种层组成，并按序列执行，各层可以认为相对独立，层与层之间全连接或者部分连接。每一个layer包括若干sub-layer，叫 feature map。总的来说，有三种layers，convolutional 以及pooling layers，在网络顶层有一个由若干layer组成的classifier。对于浅层网络MLP来说，由输入层，输出层以及若干隐含层组成。而深度神经网络的隐含层可以有各种变化，诸如RBM，LRN，CONV，MLP，POOLING等。在顶层是由若干层组成的分类器(果断时间好好整理整理深度学习模型，再作具体研究，本文只专注于论文内容，作适当延伸)。	</p>

<h4 id="section-3">卷积层</h4>
<p>  	卷积层的作用，对输入特征做映射，因此输入输出feature map之间并不是全连接。设想输入是一个图片，卷积是一个2D变换，输入图片的一个子区域Kx<em>Ky，以及一个相同维数的变换矩阵。变换矩阵(kernel values)是输入输出之间卷积层的突触权重。一个输入层通常包含多个feature map，而输出feature map通常通过对同样窗口大小的输入feature map做卷积得到。对于一个卷积层，pooling层，分类器级联的网络，kernel是3D，Kx</em>Ky*Ni，Ni是输入的feature map数目，在一些情况下，连接是稀疏的，即不是所有的输入feature map都对最终的输出feature map有贡献。通常一个非线性函数用于卷积层输出，例如tanh(x).</p>

<h4 id="pooling">POOLING层</h4>
<p>  Pooling层的作用，是统计临近数据集合的信息。以图像为例，倾向于只包含给定窗口大小或者一定尺度的显著的图像信息。重要的作用就是降低feature map的维数。每一个feature map都是独立的进行pooling，即2D pooling，而不是3D。卷基层和pooling层交替结合成为一个深度结构，顶层通常是一个分类器，线性或者是两层感知器。分类器通常聚集所有feture map，故而在这一层没有feature map的概念。</p>

<h3 id="section-4">小型神经网络的硬件实现</h3>
<p>  最自然的把神经网络映射到硬件上的想法就是所有的神经元和突触都布局在硬件上，内存只是用来输入数据以及储存结果。简单地把软件神经元/突触全映射，所以硬件实现可以匹配概念上的神经网络。每个神经元都由逻辑电路实现，突触由锁存器或者RAM实现。这样地结构可以满足一些神经元与突触数目较小的小型神经网络，提供较大的速率与较小的功耗，因为数据的移动距离较小（比起从内存搬数据）。无论对于从一层到另一层的神经元，或者对于从一个突触锁存到相应神经元，都是如此。这样的结构，对于90-10-10的感知器来说，执行时间15ns，能耗为通用处理器的1/974. </p>

<p>  但是，面积，功耗，延迟都是随着神经元数目呈二次方增长。我们综合出了一个ASIC神经网络，算出了它们的面积，延时，功耗。每个神经元执行如下操作，将输入神经元与突触相乘，将神经元对应的这些和进行相加，然后进行sigmoid变换。16<em>16 layer需要0.71mm，32</em>32 layer需要2.66mm。设想当神经元是成千上万的时候，一个硬件全映射的神经网络，面积是不可想像的，所以这样的实现对大型神经网络并不适合。</p>
<figure>
	<img src="http://mhs-blog.qiniudn.com/map.jpg" alt="" />
	<figcaption>软硬件神经元一一映射神经网络的 能耗-输入/输出神经元数目 关系图.</figcaption>
</figure>

<h3 id="section-5">寒武纪1号大型神经网络协处理器实现</h3>
<p>####整体架构
  寒武纪1号包括4个主要模块，控制处理器模块（CP）负责取指令，并配置
整个加速器的运行。神经网络计算功能部件（NFU）负责具体的神经网络计算。两个神经元缓存（NB）暂存正在处理的神经层及其依赖的神经层上神经元的数据。此外，还有一个突触缓存（SB）负责暂存正在处理的神经层及其依赖的神经层之间突触的数据。 </p>
<figure>
	<img src="http://mhs-blog.qiniudn.com/hanwuji-1.jpg" alt="" />
	<figcaption>寒武纪1号整体架构图.</figcaption>
</figure>

<h4 id="ram-">将神经元/突触权值分布式存储于片上 RAM ，优化数据搬运</h4>
<p>  在运算开始阶段，突触/神经元初始权值通过主内存分别传入 SB 以及 NBin 两个 Buffer 中；运算阶段，通过控制处理器的指令，将相应的突触/神经元权值取到运算部件 NFU 中执行，并将执行结果送回 Buffer ；神经网络运算完毕，再将运算结果传回主内存。即神经网络运算过程中，只有 SB, NBin 以及 NBout 三个 Buffer与运算部件 NFU 有数据交互，并且为每个 Buffer 分别配置一个 DMA，并且独立存储指令请求，可以极大限度地进行指令预取而不用担心冲突情况的发生，有效地降低了访存延迟。相对于传统的 CPU/GPU 架构，数据搬运消耗只是采用通用处理器基于 Cache 层次数据搬运次数的1/30 ~ 1/10，极大限度缩短了数据传输路径长度。</p>

<p>  不同数据权值的分块存储，好处有如下两点。首先，避免了不同数据类型混在一块，造成的带宽利用率不高/数据读写效率不高的问题。比如，NBin 以及 NBout 的宽度为 Tn<em>2 Bytes, SB 的宽度为 Tn</em>Tn<em>2 Bytes. 如果数据存储在一块，当设计为 Tn</em>2 Bytes 的宽度，则SB的值无法一次取出，增加了延迟。当设计为Tn<em>2 Bytes, 会造成很大的带宽浪费，增加了能耗。其次，节省了解决冲突问题所需要的消耗。为了降低延迟，当数据不加区分存储在 Cache 中，Cache端口带宽必须设计为Tn</em>Tn*2 Bytes，则Cache的面积太大，为了解决冲突，需要增加更多的判断逻辑，会增加功耗或者延迟，降低了神经网络运算效率。</p>

<h4 id="buffer">旋转Buffer设计，软件神经元复用硬件神经元，支持任意规模神经网络</h4>
<p>  为了解决软件神经元复用硬件神经元，将 NBin Buffer 设计成了一个旋转Buffer，神经网络各层的输入权值在Buffer中进行分块存储，使用完毕即覆盖。则随着神经网络层次深度的增加，NBin Buffer也有能力处理。通过复用硬件神经元的方式，使得寒武纪神经网络协处理器可以支持任意规模的深度学习算法。</p>

<h4 id="section-6">线性插值实现传输函数，高可配置性</h4>
<p>  NFU运算单元为三层设计，第一层为突触/神经元权值地乘法操作，第二层为加法树，实现部分和。第三层为传输函数地实现，通过线性插值的方式实现，因而可以实现诸如sigmoid的各种非线性传输函数。这样的分层次运算单元结构，通过指令的配置，可以模拟 MLP, CONVOLUTION, POOLING, LRN, RBM等各种形态的神经网络传输层，保证了神经网络的高可配置性。 </p>

<h4 id="bit-320bit-">16-bit 定点运算代替320bit 浮点运算</h4>
<p>  保证精度的同时，降低运算部件的复杂度，降低面积以及功耗。在运算部件的设计中，我们发现，当使用32-bit 的浮点操作与16-bit 的定点操作，对于 UCI 基准数据集，误差率分别为0.0311以及0.0337，误差率相差不大。而16-bit定点乘法器面积和功耗均为32-bit浮点乘法器的近1/7，由此可见，在能够保证正确率的情况下，使用16-bit的定点运算部件，是一个更好的选择。这也在一定程度上降低了我们设计的面积以及功耗。</p>

<h3 id="summary">Summary</h3>
<p>  看论文过程中大概纪录了一下关键点。感觉这个设计，最大的创新点，就是片山分块存储不同的权值到Buffer中，在保证了带宽使用效率的同时，还避免了考虑Cache冲突的问题。以及旋转Buffer的设计，使得软件神经元可以复用硬件神经元( 这点还没怎么看明白)。文章的核心内容就是，要使得突触权值尽量靠近要使用它的神经元。关于定点运算模拟浮点运算的操作，以及使用线性插值生成各种传输函数这两点，应该是很基本的，自己原来都不了解，还是功力不够。</p>

  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    <div class="entry-meta">
      <span class="entry-date date published updated"><time datetime="2014-10-22T00:00:00-04:00"><a href="http://lambda.hk/machine_learning/Neural-Network-Accelerator-I/">October 22, 2014</a></time></span><span class="author vcard"></span>&nbsp; &bull; &nbsp;<span class="entry-comments"><a href="http://lambda.hk/machine_learning/Neural-Network-Accelerator-I/#disqus_thread">Comment</a></span>
      
    </div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://lambda.hk/machine_learning/Neural-Network-Accelerator-I/" rel="bookmark" title="Neural Network Accelerator I" itemprop="url">Neural Network Accelerator I</a></h1>
    
  </header>
  <div class="entry-content">
    <h3 id="section">写在前面</h3>
<p>  这段时间由于工作需要，研究了一下ASPLOS’14的Best Paper，”Dianao: A Small-Footprint High-Throughput Accelerator for Ubiquitous Machine-Learning.” 这是中国大陆的学者首次获得 CCF 推荐的体系结构领域 A 类会议的最佳论文奖。而此前,ASPLOS 最佳论文 奖一直被美国卡耐基梅隆大学、德州大学奥斯丁分校、微软等 8 个欧美著名研究机构所垄断。写两篇博文，纪录一下可以深度学习硬件协处理器的相关内容。本文先介绍从浅层学习到深度学习，以及由此引起的硬件瓶颈问题，以及各种搭建神经网络运算平台的解决方案的劣势。</p>

<h3 id="section-1">从浅层学习到深度学习.</h3>
<p>  机器学习，即通过算法，使得机器能从大量历史数据中学习规律，从而对新的样本做智能识别或对未来做预测。从1980年代末期以来，机器学习的发展大致经历了两次浪潮：浅层学习（Shallow Learning）和深度学习（Deep Learning）。</p>

<p>  80年代末期，用于人工神经网络的反向传播算法（BP算法）的发明，掀起了基于统计模型的机器学习热潮。人们发现，利用BP算法可以让一个人工神经网络模型从大量训练样本中学习出统计规律，从而对未知事件做预测。这种基于统计的机器学习方法比起过去基于人工规则的系统，在很多方面显示出优越性。这个时候的人工神经网络，虽然也被称作多层感知机（MLP），但实际上是一种只含有一层隐层节点的浅层模型。</p>

<p>  90年代，各种各样的浅层机器学习模型相继被提出，比如SVM、Boosting、最大熵方法（Logistic Regression）等。这些模型的结构基本上可以看成带有一层隐层节点（如SVM、Boosting），或没有隐层节点（如LR）。这些模型在无论是理论分析还是应用都获得了巨大的成功。相比较之下，由于理论分析的难度，加上训练方法需要很多经验和技巧，所以这个时期浅层人工神经网络反而相对较为沉寂。</p>

<p>  2000年以来互联网的高速发展，对大数据的智能化分析和预测提出了巨大需求，浅层学习模型在互联网应用上获得了巨大成功。最成功的应用包括搜索广告系统（比如Google的AdWords、百度的凤巢系统）的广告点击率CTR预估、网页搜索排序（例如Yahoo!和微软的搜索引擎）、垃圾邮件过滤系统、基于内容的推荐系统等。</p>

<p>  2006年，加拿大多伦多大学教授、机器学习领域泰斗——Geoffrey Hinton和他的学生Ruslan Salakhutdinov在顶尖学术刊物《科学》上发表了一篇文章，开启了深度学习在学术界和工业界的浪潮。这篇文章有两个主要的信息：1. 很多隐层的人工神经网络具有优异的特征学习能力，学习得到的特征对数据有更本质的刻画，从而有利于可视化或分类；2. 深度神经网络在训练上的难度，可以通过“逐层初始化”（Layer-wise Pre-training）来有效克服，在这篇文章中，逐层初始化是通过无监督学习实现的。</p>

<h3 id="section-2">更复杂的模型，更精确的刻画</h3>
<p>  浅层模型有一个重要特点，就是依靠人工经验来抽取样本的特征，而强调模型主要是负责分类或预测。在模型的运用不出差错的前提下，特征的好坏就成为整个系统性能的瓶颈。因此，通常一个开发团队中更多的人力是投入到发掘更好的特征上去的。要发现一个好的特征，就要求开发人员对待解决的问题要有很深入的理解。而达到这个程度，往往需要反复地摸索，甚至是数年磨一剑。因此，人工设计样本特征，不是一个可扩展的途径。</p>

<p>  深度学习的实质，是通过构建具有很多隐层的机器学习模型和海量的训练数据，来学习更有用的特征，从而最终提升分类或预测的准确性。所以“深度模型”是手段，“特征学习”是目的。区别于传统的浅层学习，深度学习的不同在于：1. 强调了模型结构的深度，通常有5层、6层，甚至10多层的隐层节点；2. 明确突出了特征学习的重要性，也就是说，同过逐层特征变换，将样本在原空间的特征表示变换到一个新特征空间，使分类或预测更加容易。在大数据情况下，只有比较复杂的模型，或者说表达能力强的模型，才能充分发掘海量数据中蕴藏的丰富信息。</p>

<h3 id="section-3">深度学习带来的硬件门槛问题</h3>
<p>  对于互联网公司而言，如何在工程上利用大规模的并行计算平台来实现海量数据训练，是各家公司从事深度学习技术研发首先要解决的问题。目前广泛采用的方法，都是搭建 CPU 集群，搭载大数据平台如Hadoop，或者通过GPU运行并行算法，进行处理。由于数据处理的延迟太高，显然不适合需要频繁迭代的深度学习。现有成熟的深度学习训练技术大都是采用随机梯度法（SGD）方法训练的。这种方法本身不可能在多个计算机之间并行。</p>

<p>  即使是采用 GPU 进行传统的DNN 模型进行训练，其训练时间也是非常漫长的，一般训练几千小时的声学模型需要几个月的时间。而随着互联网服务的深入，海量数据训练越来越重要，DNN 这种缓慢的训练速度必然不能满足互联网服务应用的需要。Google搭建的DistBelief，是一个采用普通服务器的深度学习并行计算平台，采用异步算法，由很多计算单元独立地更新同一个参数服务器的模型参数，实现了随机梯度下降算法的并行化，加快了模型训练速度。但 Google 通过1.6万片 CPU 核处理器，10亿个内部节点构成的庞大系统，通过1000万帧图片，对其训练了 10 天，才实现了猫脸识别的无监督学习。</p>

<p>  还有的公司，利用FPGA的高可配置性，使用FPGA集群来运行神经网络算法。然而，无论是 CPU, GPU, 还是FPGA, 对于大型深度神经网络来说，都价格太昂贵，而且受限于访存瓶颈，训练代价太大。这些方法，都无法满足深度神经网络发展的需要。</p>

<p>  而自上世纪80/90年代，神经网络算法萌芽后，也一直有体系结构研究者，试图针对神经网络算法，研制专用协处理器。但从已有成果来看，或者是进行硬件神经元与软件神经元的一一映射，解决了访存的瓶颈，但这样的架构，满足不了深度神经网络随着网络复杂产生的参数爆炸性增长问题。进行网络在芯片上的全定制，对于浅层网络，还有应用的空间，但对于深度神经网络，不堪重负。</p>

<h3 id="cpugpu">CPU/GPU解决方案的劣势</h3>
<p>  无论 GPU 还是 CPU 集群，都将神经网络从软件层面映射到运算逻辑单元，突触/神经元权值存储于主内存。而深度神经网络，层次多，层与层之间的连接权值较多，逐层运算，需要不断地进行突触/神经元权值的存取与更新。随着神经网络规模的增大，连接权值增多，访存速度成为了制约深度神经网络规模增大的瓶颈。而传统架构从主内存到运算部件的通路设计，中间经过多级Cache，突触与神经元连接权值需要分别从主内存取得，在带来访存速度瓶颈以及延时问题的同时，也会增加更多的功耗。而通用处理器为了满足其通用性，相对神经网络算法来说，会存在太多的冗余单元。这些都不利于使用CPU/GPU来搭建大规模的神经网络运算平台。</p>

<figure>
	<img src="http://mhs-blog.qiniudn.com/cpu_ram.jpg" alt="" />
	<figcaption>通用处理器从核心到主内存通路.</figcaption>
</figure>

<h3 id="section-4">软硬件神经元全映射神经网络协处理器的劣势</h3>
<p>  在此之前有的神经网络协处理器，针对浅层神经网络定制，同层网络中的软件神经元与硬件神经元多为一一映射的关系，每个神经元用一个逻辑电路表示，突触使用一个锁存器/RAM实现。这样的设计，对于浅层网络，可以带来很大的的加速比。然而，软件神经元无法复用硬件神经元，随着神经元的增长，面积，功耗，延时都会以平方倍的关系增长。这样的协处理器，难以处理深度学习这样的大规模神经网络。下图所示，为软件/硬件神经元一一映射的协处理器结构：</p>
<figure>
	<img src="http://mhs-blog.qiniudn.com/full.jpg" alt="" />
	<figcaption>软硬件神经元全映射协处理器.</figcaption>
</figure>

<h3 id="section-5">寒武纪1号神经网络协处理器解决方案</h3>
<p>  寒武纪1号单核神经网络协处理器， 将神经元/突触权值分布式存储于片上 RAM ，优化数据搬运，解决了访存瓶颈问题；并且通过硬件神经元分时复用，能够满足各种规模的深度神经网络。在台积电 65nm 工艺下,寒武纪 1 号主频可达 0.98GHz、性能 452 GOPS、功耗 0.485W、面积约为 3mm2。也就是说，它可以在通用处理器核 1/10 的面积和功耗开销下，达到通用处理器核 100 倍以上的人工神经网络处理速度。即便和最先进的 GPU 相比, 寒武纪 1 号的人工神经网络处理速度也不落下风，而其面积和功耗远低于 GPU 的 1/100。在下一篇文章中，我将就着论文内容，详细介绍寒武纪1号。</p>

<h3 id="reference">Reference.</h3>
<p>  本文参考了 <a href="http://www.csdn.net/article/2013-05-29/2815479">深度学习： 推进人工智能的梦想</a>，在此表示感谢。</p>

  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    <div class="entry-meta">
      <span class="entry-date date published updated"><time datetime="2014-10-08T00:00:00-04:00"><a href="http://lambda.hk/c++/C-parameter/">October 08, 2014</a></time></span><span class="author vcard"></span>&nbsp; &bull; &nbsp;<span class="entry-comments"><a href="http://lambda.hk/c++/C-parameter/#disqus_thread">Comment</a></span>
      
    </div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://lambda.hk/c++/C-parameter/" rel="bookmark" title="C++ 参数传递" itemprop="url">C++ 参数传递</a></h1>
    
  </header>
  <div class="entry-content">
    <h3 id="section">参数传递</h3>
<p>  这是在前几天看C++ Primer 的第8章时做的笔记，之前大致翻阅过C++ Primer,很多知识点，看过，也就忘记了。最近工程上，接触C++比较多，在做的过程中，再回过头来细细回顾这本经典，很多原来模糊的东西，都变得明晰，而很多所以然，也渐渐知晓。</p>

<h4 id="section-1">1. 常量引用</h4>

<div class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="n">func</span><span class="p">(</span><span class="k">const</span> <span class="kt">double</span> <span class="o">&amp;</span><span class="n">ra</span><span class="p">)</span></code></pre></div>

<ul>
  <li>当引用传递，又不想函数改变传进去的值时，可以使用常量引用，这种可以直接传递使用值。</li>
  <li>普通值传递相当于是传递数据的副本，即需要复制原始结构的拷贝，会另外生成一份数据，故数据量大时，还是引用传递较为节省内存。</li>
</ul>

<h4 id="section-2">2. 返回引用值</h4>

<div class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="n">free_throws</span> <span class="o">&amp;</span> <span class="n">accumulate</span><span class="p">(</span><span class="n">free_throws</span> <span class="o">&amp;</span> <span class="n">target</span><span class="p">,</span> <span class="k">const</span> <span class="n">free_throws</span> <span class="o">&amp;</span> <span class="n">source</span><span class="p">)</span> <span class="p">{</span>
	<span class="k">return</span> <span class="n">target</span><span class="p">;</span>
<span class="p">}</span></code></pre></div>

<ul>
  <li>将引用变量target传入了函数，该变量是可以被修改的，再返回对它的引用，如果函数头没有&amp;，将返回对target指向变量的拷贝。</li>
  <li>传统的返回机制，与按值传递函数参数类似，被调用函数返回在函数体内定义的临时变量时，该变量return以后存在一个临时存储单元中，再复制给上层调用函数的接收变量。</li>
  <li>当返回为引用时，避免生成临时变量，效率更高。</li>
  <li>函数体内定义的为临时变量，应该避免返回对临时变量的引用。当试图返回对临时变量的引用，子函数执行完毕，其声明的临时变量内存将被释放，即试图引用已经释放的内存，会引起程序奔溃。</li>
</ul>

<h4 id="section-3">3.形参实参不匹配</h4>
<ul>
  <li>形参实参不匹配：当将int实参传递给const double &amp; 形参时，实参与形参类型不匹配，但是形参可以转变成引用类型（实参），程序将创建一个正确类型的临时变量，使用转换后的实参值来初始化它。然后传递一个指向该临时变量的引用。</li>
</ul>

<h4 id="section-4">4. 基类引用</h4>
<ul>
  <li>基类引用可以指向基类对象，也可以指向派生类对象，而无需进行强制类型转换。</li>
</ul>

<h4 id="section-5">5. 对于使用传递的值而不做修改的函数</h4>
<ul>
  <li>数据对象很小，如内置，则按值传递</li>
  <li>数据对象是数组，只能使用指针传递，使用const指针</li>
  <li>数据对象是较大结构，使用const指针或者const引用</li>
  <li>类对象，使用const引用，这是C＋＋新增引用特性的原因</li>
</ul>

<h4 id="section-6">6. 对于修改传递数据的函数</h4>
<ul>
  <li>内置数据类型，使用指针</li>
  <li>数组，使用指针</li>
  <li>结构体，使用引用或者指针</li>
  <li>类对象，使用引用</li>
</ul>

<h4 id="section-7">7. 带默认值参数的函数</h4>

<div class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="kt">char</span><span class="err">＊</span> <span class="n">left</span><span class="p">(</span><span class="k">const</span> <span class="kt">char</span> <span class="o">*</span><span class="n">str</span><span class="p">,</span> <span class="kt">int</span> <span class="n">n</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span></code></pre></div>

<ul>
  <li>该函数返回一个新的字符串，将其返回类型设置为指向char的指针，希望原始字符串保持不变，加入const限定符，希望n的默认值为1，当传递n值，将覆盖1。</li>
  <li>当为某个参数设置默认值，必须将其右边的所有参数都设置默认值。</li>
  <li>当被调用函数返回的是指针或者引用，又不是传入的参数引用，则在该函数体内使用new创建了新的内存空间，在使用完返回的引用或者指针时，应该通过返回的指针/引用将其删除。</li>
</ul>

  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    <div class="entry-meta">
      <span class="entry-date date published updated"><time datetime="2014-10-08T00:00:00-04:00"><a href="http://lambda.hk/c++/C-Array/">October 08, 2014</a></time></span><span class="author vcard"></span>&nbsp; &bull; &nbsp;<span class="entry-comments"><a href="http://lambda.hk/c++/C-Array/#disqus_thread">Comment</a></span>
      
    </div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://lambda.hk/c++/C-Array/" rel="bookmark" title="C++ 数组初始化" itemprop="url">C++ 数组初始化</a></h1>
    
  </header>
  <div class="entry-content">
    <h3 id="section">数组初始化</h3>
<p>  之前对C++的数组，特别是二维数组初始化，就有点混乱，每次有问题，上网查一下，回头还是一样的状态。前段时间刷Leetcod，做Palindrome Partitioning时，顺便查资料做了一个总结，方便以后查阅。</p>

<h4 id="int-ptr-ptrnn">1. int **ptr 声明指向一群指向整数指针的指针，作类比，即指向地址ptr[N][N]，不知这样类比是否合适？</h4>
<ul>
  <li>由于是指针的指针，所以需要两次分配内存才能最终使用。</li>
  <li>第一次分配内存：
          int <em>*ptr = new int</em>[5]，即5个指向N个指针的指针，此时与2等价。</li>
  <li>第二次分配内存：
         循环体内，ptr[i] = new int[N]，分别对5个指针分配内存。</li>
  <li>释放内存操作：
         循环体内delete [] ptr[i]，删除每行的对象，结束循环后，
         delete [] ptr，释放第一次分配的N个指针内存。</li>
  <li>函数参数传递：
          函数参数传递时，实参直接用ptr，形参用int  **f指针。</li>
</ul>

<h4 id="int-ptr5-5ptr5n">2. int* ptr[5] 声明5个指向整数指针的指针，作类比，即指向地址ptr[5][N]</h4>
<ul>
  <li>分配内存直接按1中第二次分配内存的方式。这种声明方式不能用一个变量len代替5的位置，在编译阶段变量并没有值，无法开辟空间。</li>
  <li>内存释放操作与1中一样。</li>
</ul>

<h4 id="int-ptr5-5ptrn5">3. int (*ptr)[5] 声明指向一群指向5个整数数组的指针，作类比，即指向ptr[N][5]</h4>
<ul>
  <li>如果想分配N个指针内存空间，ptr ＝ (int (<em>)[5]) new int[5</em>N]，则ptr[i]指向第i个5个整数数组的首地址。</li>
</ul>

<h4 id="int-ann--0">4. 直接将二维数组作为参数传递，数组定义：int a[n][n] = {0}</h4>
<ul>
  <li>由于编译器寻址为p+i*n+j, 故不能省略第二维的大小，这种情况下n一定要是确定值，不能使用变量</li>
  <li>形参：int **a</li>
  <li>实参：(int**)a</li>
  <li>函数内对数组寻址：<em>((int</em>)array + n*i + j)</li>
</ul>

  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    <div class="entry-meta">
      <span class="entry-date date published updated"><time datetime="2014-10-07T00:00:00-04:00"><a href="http://lambda.hk/machine_learning/Machine-Learning-Course-Summary/">October 07, 2014</a></time></span><span class="author vcard"></span>&nbsp; &bull; &nbsp;<span class="entry-comments"><a href="http://lambda.hk/machine_learning/Machine-Learning-Course-Summary/#disqus_thread">Comment</a></span>
      
    </div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://lambda.hk/machine_learning/Machine-Learning-Course-Summary/" rel="bookmark" title="Machine Learning Course Summary" itemprop="url">Machine Learning Course Summary</a></h1>
    
  </header>
  <div class="entry-content">
    <h3 id="where-it-from">Where it from</h3>
<p>  这份summary是研一上学期上卿老师的机器学习时候做的总结。考试前几个人成立了个讨论组，借着CSDN上面的一些博文，比如 <a href="http://blog.csdn.net/abcjennifer/">浙大的研究生Rachel Zhang</a> ，并把课件内容做了一个总结，后来，这份资料就不知道哪里去了。记得当时就是按着课件内容，大略做了一个记录，对Machine Learning一直是云里雾里，直到后来，参加了阿里的大数据竞赛，之后又做了一个基于神经网络模型的时间序列数据分析方面的工作，以及一个kaggle的ctr方面的竞赛，才对ML的模型，特征有了一个相对清晰的认识。有一次尹叔准备阿里实习生的面试，还问我要过这一份资料，直到最近，偶然又从移动硬盘里翻了出来，Machine Learning的道路，要继续往前走，而这，仅仅当作对逝去光阴的缅怀。</p>

<h3 id="text-classification">Text Classification.</h3>
<p>  分类所需的类别体系预先设定,且长时间不改变。一篇文档分配给不同类别,置 信度不同。人类的判断大多依据经验以及直觉,如何让机器像人类一样自己来通 过对大量同类文档的观察来自己总结经验,作为今后分类的依据。这便是统计学 习方法,亦是机器学习的基本思想。其中需要一批由人工进行了准确分类的文档 作为学习的材料,计算机从这些文档重挖掘出一些能够有效分类的规则,这个过 程被称为训练,而总结出的规则集合常常被称为分类器。训练完成之后,需要计 算机用该分类器对于从来没有见过的文档进行分类时。</p>

<p>  进行文本分类时,不光文本中是包含哪些词很重要,这些词出现的次数对分 类也很重要。这一前提使得向量模型(俗称的VSM,向量空间模型)成了适合文 本分类问题的文档表示模型。在这种模型中,一篇文章被看作特征项集合来看, 利用加权特征项构成向量进行文本表示,利用词频信息对文本特征进行加权。当 特征集过大,容易引起过拟合。通过特征提取或特征重构降维。
讨论的几种分类算法:朴素贝叶斯、支持向量机、k-NN、线性回归、逻辑回 归。</p>

<h3 id="naive-bayes">Naive Bayes</h3>

<ul>
  <li>
    <p>关注文档属于某类别的概率。文档属于某个类别的概率等于文档中每个词属于该 类别的概率的综合表达式。而每个词属于该类别的概率又在一定程度上可以用这 个词在该类别训练文档中的词频信息来粗略估计,使用朴素贝叶斯算法时,在训 练阶段的主要任务就是估计这些值。</p>
  </li>
  <li>
    <p>[P(C_i|D)=\frac{P(D|C_i)P(C_i)}{P(D)}=\frac{P(w_1|C_i)P(w_2|C_i)…P(w_n|C_i)}{P(D)}]</p>
  </li>
  <li>
    <p>朴素贝叶斯的缺陷:成立的前提条件是各特征条件独立,使用某个特征值在类别中出现的数目来估计P(w_i|C_i),只有在训练样本非常多的情况下结果才会准确。</p>
  </li>
</ul>

<h3 id="k-nn-classification">k-NN Classification</h3>
<p>  在kNN算法看来,训练样本代表类别的准确信息(基于实例的分类器),而不管 样本是使用什么特征表示的。其基本思想是在给定新文档后,计算新文档特征向 量和训练文档集中各个文档的向量的相似度,得到K篇与该新文档距离最近最相 似的文档,根据这K篇文档所属的类别判定新文档所属的类别,即该算法不存在 真正意义上的训练阶段。kNN唯一的也可以说最致命的缺点就是判断一篇新文档 的类别时,需要把它与现存的所有训练文档全都比较一遍,计算代价太大。</p>

<h3 id="supervised-learning--unsupervised-learning">Supervised Learning &amp;&amp; Unsupervised Learning</h3>
<p>  监督学习:学习过程中使用的样例是由输入/输出对给出时。最典型的监督学习例 子就是文本分类问题,训练集是一些已经明确分好了类别文档组成,文档就是输 入,对应的类别就是输出。非监督学习:学习过程中使用的样例不包含输入/输出对,学习的任务是理解数据产生的过程。典型的非监督学习例子是聚类,类别的 数量,名称,事先全都没有确定,由计算机自己观察样例来总结得出。
。</p>

<h3 id="overfitting--">Overfitting &amp;&amp; 线性可分</h3>
<p>  为了得到一致假设而使假设变得过度复杂称为过拟合。想像某种学习算法产生了 一个过拟合的分类器,这个分类器能够百分之百的正确分类样本数据,但也就为 了能够对样本完全正确的分类,使得它的构造如此精细复杂,规则如此严格,以 至于任何与样本数据稍有不同的文档它全都认为不属于这个类别,即扩展性差。 如果存在一个超平面能够正确分类训练数据,并且这个程序保证收敛,这种情况 称为线形可分。如果这样的超平面不存在,则称数据是线性不可分。过拟合解决办法:减少特征数量,可以选用模型选择法;正则化,即保留所 有特征,但降低参数值的影响。</p>

<h3 id="section">特征提取算法:开方检验与信息增益</h3>
<ul>
  <li>
    <p>在特征提取时,选择相关性较大的量，可以通过开方检验或者计算信息增益</p>
  </li>
  <li>
    <p>开方检验 (\sum_{i=1}^n\frac{(x_i-E)^2}{E}) ,小于事先设定的阀值,则假设成立,否者,假设不成立。</p>
  </li>
  <li>
    <p>信息增益,信息熵的定义: (H(X)=-\sum<em>{i-1}^nP_i\log_2P_i \qquad H(C|X)=\sum</em>{i=1}^{n}P_iH(C|X=x_i))</p>
  </li>
</ul>

<h3 id="section-1">单一参数线性回归</h3>
<p>  考虑用一条过原点的直线去拟合采样点,(Y=W^TX),未知参数w取什么值可以使 得拟合最好,这是一个最小二乘法拟合问题。目标是使(\sum(Y_i^*-Y_i)^2) 最小。通过 样本观察值去估算最好的W,有MLE和MAP两种方法。通过求偏导可求得最好 的W.</p>

<h3 id="section-2">多项式拟合</h3>
<ul>
  <li>
    <p>拟合函数：(Y(X,W)=\sum_{j=0}^{M}W_jX^j)</p>
  </li>
  <li>
    <p>目标函数：(E(W)=0.5\sum_{i=0}^N{y(x_n,w)-t_n}^2)</p>
  </li>
  <li>
    <p>过拟合：曲线和采样观察点拟合的很好但是却偏离了整体,称为过拟合,即对于多项式, 阶数越大,对采样点拟合越好,但是对噪声越敏感。而我们的终极目标是对于给 定的新数据,能够做出精确的预测,应该避免过拟合。随着拟合多项式阶数的增 大,训练误差减小,而测试误差先减小,后增大。对于过拟合,若增加训练样 本,可以得到缓解。</p>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>正则化：最小二乘法与MLE是一致的,采用贝叶斯方法,可以避免过拟合现象,由于 有效参数个数可以随着数据集的大小动态调整。从最小二乘法的角度,为了解决过度拟合的问题,我们可以改变优化目标, 加入正则化,限制W的值过大 [E(W)=0.5\sum_{i=0}^N(y(x_n,w)-t_n)^2+0.5\lambda</td>
          <td>W</td>
          <td>]</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>最小二乘法求解,即直接由数学公式求解, (\theta=(X^TX)^{-1}X^TY)</li>
</ul>

<h3 id="section-3">多元线性回归</h3>
<ul>
  <li>
    <p>输入为向量(包含多个特征分量),假如知道加在每个数据点上的噪声的方差,可以由MLE估计W</p>
  </li>
  <li>
    <p>假定 [Y_i~N(\omega x_i,\delta_i^2)] ，求[\mbox{argmax(MLE)=argmin}\sum_{i=1}^R\frac{(y_i-\omega x_i)^2)}{\delta_i^2}]</p>
  </li>
</ul>

<h3 id="section-4">逻辑回归</h3>
<p>  逻辑回归模型,是一个非线性函数,sigmoid函数,可以轻松处理0/1分类问题 如果直接应用线性回归来拟合 逻辑回归数据,就会形成很多局部最小值。是一个 非凸集,而线性回归损失函数 是一个 凸函数,即最小极值点,即是全局极小点。 模型不符。若采用逻辑回归的损失函数,损失函数就能形成一个凸函数。 在线性回归中,若使模型选择与测量数据最接近,则其概率积最大,则形成一个 极大似然估计,推导即得平方和最小公式;而逻辑回归采用对数形式。</p>

<h3 id="section-5">正则化损失函数</h3>
<ul>
  <li>
    <p>线性回归：    [J(\theta)=\frac{1}{2m}[\sum<em>{i=1}^{m}(h</em>{\theta}(x^{(i)})-y^{(i)})^2+\lambda\sum_{j=1}^{n}\theta_j^2]]</p>
  </li>
  <li>
    <p>逻辑回归：[J(\theta)=-\frac{1}{m}[\sum{i=1}{m}y^{(i)}\log{h<em>{\theta}(x^(i))}+(1-y^{(i)})(1-y^{(i)})\log{(1-h</em>{\theta}(x^{(i)}))}]+\frac{\lambda}{2m}\sum_{j=1}^{m}\theta_j^2]</p>
  </li>
  <li>
    <p>对损失函数正则化处理,相当于加上一个奥卡姆剃刀,把原本复杂的系数变 得简单,解决由于维数增加导致的系数膨胀问题。</p>
  </li>
  <li>
    <p>正则化,即给参数加惩罚项,当给不需要的参数加大的惩罚项时,为求使得 损失函数最小的参数,则惩罚项大的参数将趋于0.从贝叶斯估计的角度来看,正则化项对应于模型的先验概率。可以假设复杂的模型有较大的先验概率,简单的 模型有较小的先验概率。</p>
  </li>
</ul>

<h3 id="section-6">模型选择</h3>
<ul>
  <li>
    <p>bias:多次训练所得的曲线的均值与实际最佳拟合情况之间的差值。 </p>
  </li>
  <li>
    <p>variance:多次训练所得的曲线与实际最佳拟合情况之间的差值,即方差。 </p>
  </li>
  <li>
    <p>欠拟合时,偏差大,即high bias,训练误差大,测试误差大;过拟合时,方差大, 即high variance,训练误差小,测试误差大(测试误差是个先减小后增大的过程)。</p>
  </li>
  <li>
    <p>variance:估计本身的方差。bias:估计的期望和样本数据样本希望得到的回 归函数之间的差别。</p>
  </li>
  <li>
    <p>对于正则化中的λ,λ小 ,阶数大,overfit(flexible),对于不同的训练数据集的拟合 结果抖动很大,则variance 大,bias是估计均值与实际值期望的偏差,bias小;反 之,underfit(stable),对于不同的训练数据集的拟合结果抖动较小,则variance小, 不能很好地进行回归,bias大。</p>
  </li>
  <li>
    <p>正则化:即为了避免过拟合,在误差函数中引入的一个分量。
训练集越小,训练误差越小x,测试误差越大;训练集越大,训练误差越大, 测试误差越小,最终两者趋于相同。</p>
  </li>
  <li>
    <p>欠拟合时,增加训练集,并不能改善性能;而在过拟合状态,增加训练集, 则可以改善性能。 </p>
  </li>
  <li>
    <p>欠拟合状态可采取的措施:减小λ,增加特征维数,增加多项式特征; 过拟合状态可采取的措施:增大λ,减少特征维数,增加训练样本;</p>
  </li>
</ul>

<h3 id="section-7">稀疏编码</h3>
<ul>
  <li>
    <p>稀疏编码是一种无监督学习的方法,它用来寻找一组“超完备”基向量来更高效 地表示样本数据,该基向量能够有效地找出隐含在输入数据内部的结构与模式, 可用于特征学习。</p>
  </li>
  <li>
    <p>给定m 张nxn 的images, 希望找到一个字典,或者一组基,φi, 使得每个xi都能分解成 φi的线性组合,¬(x=\sum_{j=1}^{m}a_j\varphi_j),之所以说是sparse,是因 为aj中大部分为0,而向量a就是另一种描述image的方式,也称为feature represen- tation的另一种方法,不同于raw pixels. </p>
  </li>
  <li>
    <p>当引入了超完备基,系数ai不再由输入x唯一决定,因此引入稀疏性来解决由此产 生的退化问题。要求系数稀疏,及要求输入向量,只有很少的几个值远大于0.通 过给平方和最小加正则来定义代价函数,则可以通过控制惩罚项的参数达到稀疏 的目的。</p>
  </li>
</ul>

<h3 id="section-8">核函数</h3>
<p>  SVM即使将低维不可分的样本映射到高维空间中后就线性可分,由于最后用训练 出来的模型进行分类预测时需要求高维空间中映射特征间的内积,而核函数的功 能就是我们计算时不需要考虑高维空间的具体形式,降低了其计算复杂度。 通过定义核函数,避免显示地定义基函数(特征映射)。 核技巧:寻找一个映射X -&gt; F,使得F维数比X高,因此模型更丰富,算法只需 要计算点积,存在一个核函数,在需要计算点积的地方,都用相应核函数值替 代。 核函数构造方法:可以显示地定义一个特征映射,从而得到核函数的间接定义;或者直接定义,此时需要保证核函数为有效核。 直接定义时:多项式核,Sigmoid核,RBF核,指数平方/高斯核,余弦相似度 核,字符串核……非线性核有时能极大提升分类器性能,但在训练及预测时训练 复杂度高。</p>

<h3 id="svm">SVM</h3>
<p>  SVM的目的:找到最大间隔的超平面,SVM是一种分类方法,一种监督学习方法</p>

<h3 id="boosting">Boosting</h3>
<ul>
  <li>
    <p>一种贪心的自适应基展开算法,将一些弱规则组合得到最后的强规则。每次循环 后提高误分样本的分布概率,即提高其在训练样本中的权重,使得下一次循环的 弱分类机能够集中处理这些样本。最后将弱分类机进行组合,精度高的所占权重 大。</p>
  </li>
  <li>
    <p>训练误差为0后,测试误差还在减小的现象:训练误差只考虑了分类是否正 确,却没有考虑分类的信度,随着循环次数的增加,训练样本的间隔会增大,因 此,最后的强分类器越大,间隔可能更大,分类器变得简单,测试误差更小。</p>
  </li>
  <li>
    <p>优缺点:Boosting亦有可能发生过拟合,但不常出现。分类快,简单且容易 实现,没有参数需要调整,灵活度高;但是其依赖于弱分类器即数据,当弱分类 器太强或者太弱,都会失败。</p>
  </li>
</ul>

<h3 id="section-9">最小二乘法</h3>
<p>  求函数取得极值的方法:最小二乘法,梯度下降法; 其中最小二乘法为纯数学的方法,其推导有一个假设,即估计值与真实值之间的 误差假设服从高斯分布。那么若回归所得估计值与真实值并非服从高斯分布,甚 至差距很大,则算出来的模型对测试样本的预测则未必正确, 而梯度下降法是按下面的流程进行的:首先对θ赋值,这个值可以是随机的,也可 以让θ是一个全零的向量。改变θ的值, 使得J (θ) 按梯度下降的方向进行减少。</p>

<h3 id="ldapca">线性判别分析LDA与主成分分析PCA</h3>
<p>  将带上标签的数据点,投影到维度更低的空间中,使得投影后的点,会形成按类 别区分,一簇一簇的情况,相同类别的点,将会在投影后的空间中更接近。 类内散度矩阵,类间散度矩阵。PCA的输入数据为不带标签的点,是一个预处理的方法,它可以将原本的数 据降低维度,而使得降低了维度的数据之间的方差最大。用于特征提取,有最大化方差法,最小化损失法两种,前者类似K-L 算法,提取差异化最大的若干个特 征,用于指导分类;而后者寻找D个正交的向量,用于特征重组。</p>

<h3 id="lasso">Lasso与岭回归</h3>
<ul>
  <li>
    <p>都是对系数的模进行限制,lasso是对模的长度进行限制,岭回归是对模的平方进 行限制。</p>
  </li>
  <li>
    <p>Lasso: 实际建模过程中通常需要寻找对响应变量最具有解释性的自变量子集―即 模型选择,以提高模型的解释性和预测精度。Lasso方法是一种压缩估计。它通过 构造一个惩罚函数得到较为精炼的模型,使得它压缩一些系数,同时设定一些系 数为零,因此保留了子集收缩的优点。 </p>
  </li>
  <li>
    <p>岭回归:是一种改良的最小二乘估计法,通过放弃最小二乘法的无偏性,以损失部 分信息、降低精度为代价获得回归系数更为符合实际、更可靠的回归方法。</p>
  </li>
</ul>

  </div><!-- /.entry-content -->
</article><!-- /.hentry -->


<div class="pagination">
  
    
      <a href="http://lambda.hk" class="btn">Previous</a>
    
  
  <ul class="inline-list">
    <li>
      
        <a href="http://lambda.hk">1</a>
      
    </li>
    
      <li>
        
          <span class="current-page">2</span>
        
      </li>
    
      <li>
        
          <a href="http://lambda.hk/page3">3</a>
        
      </li>
    
  </ul>
  
    <a href="http://lambda.hk/page3" class="btn">Next</a>
  
</div><!-- /.pagination -->

</div><!-- /#main -->

<div class="footer-wrapper">
  <footer role="contentinfo">
    <span>&copy; 2014 Pathfinder Mok. Powered by Jekyll  using the HPSTR Theme.</span>

  </footer>
</div><!-- /.footer-wrapper -->

<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="http://lambda.hk/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="http://lambda.hk/assets/js/scripts.min.js"></script>
<!--script src="http://lambda.hk/assets/js/backtop.js"></script-->


          

</body>
</html>
